{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import Keras library\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "\n",
    "#import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import string\n",
    "import re\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Plot[0]#.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in re.split(' |\\r\\n',doc):\n",
    "        word = word.lower()\n",
    "        punc = string.punctuation.replace(\"'\", \"\")\n",
    "        if '[' in word and ']' in word:\n",
    "            start = word.index('[')\n",
    "            end = word.index(']')\n",
    "            word = word[:start] + word[end+1:]\n",
    "        for i in punc:\n",
    "            word = word.replace(i, \"\")\n",
    "        wl.append(word.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34886,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Plot'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Plot[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.08117413520812988 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wordlist = []\n",
    "\n",
    "for i in df['Plot'][:100]:\n",
    "    wl = create_wordlist(i)\n",
    "    wordlist = wordlist + wl\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12993831"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_list.txt', 'rb') as f:\n",
    "    wordlist = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = wordlist[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  2712\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "# #save the words and vocabulary\n",
    "# with open('word_dict.txt', 'wb') as f:\n",
    "#     cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "seq_length = 30 # sequence length\n",
    "learning_rate = 0.001 #learning rate\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('word_dict.txt', 'rb') as f:\n",
    "#     w = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 9970\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "model built!\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 512)               6080512   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2712)              1391256   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2712)              0         \n",
      "=================================================================\n",
      "Total params: 7,471,768\n",
      "Trainable params: 7,471,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "seq_length = 30 # sequence length\n",
    "learning_rate = 0.001 #learning rate\n",
    "\n",
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hsiaoyuchiang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 8973 samples, validate on 997 samples\n",
      "Epoch 1/50\n",
      "8973/8973 [==============================] - 344s 38ms/step - loss: 7.6726 - categorical_accuracy: 0.0407 - val_loss: 6.8456 - val_categorical_accuracy: 0.0401\n",
      "Epoch 2/50\n",
      "8973/8973 [==============================] - 336s 37ms/step - loss: 6.4216 - categorical_accuracy: 0.0737 - val_loss: 6.7480 - val_categorical_accuracy: 0.0863\n",
      "Epoch 3/50\n",
      "8973/8973 [==============================] - 334s 37ms/step - loss: 6.2854 - categorical_accuracy: 0.0875 - val_loss: 6.7956 - val_categorical_accuracy: 0.0863\n",
      "Epoch 4/50\n",
      "8973/8973 [==============================] - 331s 37ms/step - loss: 6.1926 - categorical_accuracy: 0.0880 - val_loss: 6.8224 - val_categorical_accuracy: 0.0863\n",
      "Epoch 5/50\n",
      "8973/8973 [==============================] - 331s 37ms/step - loss: 6.0765 - categorical_accuracy: 0.0884 - val_loss: 6.8829 - val_categorical_accuracy: 0.0863\n",
      "Epoch 6/50\n",
      "8973/8973 [==============================] - 330s 37ms/step - loss: 5.9310 - categorical_accuracy: 0.0889 - val_loss: 6.9512 - val_categorical_accuracy: 0.0863\n",
      "Epoch 7/50\n",
      "8973/8973 [==============================] - 322s 36ms/step - loss: 5.8013 - categorical_accuracy: 0.0894 - val_loss: 6.9978 - val_categorical_accuracy: 0.0863\n",
      "Epoch 8/50\n",
      "8973/8973 [==============================] - 324s 36ms/step - loss: 5.6347 - categorical_accuracy: 0.0929 - val_loss: 7.0949 - val_categorical_accuracy: 0.0863\n",
      "Epoch 9/50\n",
      "8973/8973 [==============================] - 323s 36ms/step - loss: 5.4730 - categorical_accuracy: 0.0951 - val_loss: 7.1627 - val_categorical_accuracy: 0.0843\n",
      "Epoch 10/50\n",
      "8973/8973 [==============================] - 322s 36ms/step - loss: 5.2682 - categorical_accuracy: 0.0983 - val_loss: 7.2488 - val_categorical_accuracy: 0.0832\n",
      "Epoch 11/50\n",
      "8973/8973 [==============================] - 324s 36ms/step - loss: 5.0450 - categorical_accuracy: 0.1044 - val_loss: 7.3293 - val_categorical_accuracy: 0.0853\n",
      "Epoch 12/50\n",
      "8973/8973 [==============================] - 322s 36ms/step - loss: 4.7599 - categorical_accuracy: 0.1131 - val_loss: 7.4680 - val_categorical_accuracy: 0.0782\n",
      "Epoch 13/50\n",
      "8973/8973 [==============================] - 326s 36ms/step - loss: 4.4346 - categorical_accuracy: 0.1264 - val_loss: 7.5445 - val_categorical_accuracy: 0.0812\n",
      "Epoch 14/50\n",
      "8973/8973 [==============================] - 330s 37ms/step - loss: 4.0671 - categorical_accuracy: 0.1551 - val_loss: 7.6060 - val_categorical_accuracy: 0.0792\n",
      "Epoch 15/50\n",
      "8973/8973 [==============================] - 328s 36ms/step - loss: 3.6852 - categorical_accuracy: 0.1956 - val_loss: 7.6850 - val_categorical_accuracy: 0.0762\n",
      "Epoch 16/50\n",
      "8973/8973 [==============================] - 326s 36ms/step - loss: 3.2584 - categorical_accuracy: 0.2560 - val_loss: 7.8405 - val_categorical_accuracy: 0.0832\n",
      "Epoch 17/50\n",
      "8973/8973 [==============================] - 331s 37ms/step - loss: 2.8705 - categorical_accuracy: 0.3207 - val_loss: 7.8667 - val_categorical_accuracy: 0.0853\n",
      "Epoch 18/50\n",
      "8973/8973 [==============================] - 325s 36ms/step - loss: 2.5370 - categorical_accuracy: 0.3809 - val_loss: 7.9043 - val_categorical_accuracy: 0.0752\n",
      "Epoch 19/50\n",
      "8973/8973 [==============================] - 327s 36ms/step - loss: 2.2281 - categorical_accuracy: 0.4438 - val_loss: 7.9946 - val_categorical_accuracy: 0.0792\n",
      "Epoch 20/50\n",
      "8973/8973 [==============================] - 332s 37ms/step - loss: 2.0105 - categorical_accuracy: 0.4880 - val_loss: 8.1279 - val_categorical_accuracy: 0.0802\n",
      "Epoch 21/50\n",
      "8973/8973 [==============================] - 326s 36ms/step - loss: 1.7694 - categorical_accuracy: 0.5423 - val_loss: 8.1024 - val_categorical_accuracy: 0.0702\n",
      "Epoch 22/50\n",
      "8973/8973 [==============================] - 331s 37ms/step - loss: 1.5960 - categorical_accuracy: 0.5900 - val_loss: 8.4148 - val_categorical_accuracy: 0.0742\n",
      "Epoch 23/50\n",
      "8973/8973 [==============================] - 330s 37ms/step - loss: 1.4242 - categorical_accuracy: 0.6336 - val_loss: 8.2671 - val_categorical_accuracy: 0.0722\n",
      "Epoch 24/50\n",
      "8973/8973 [==============================] - 329s 37ms/step - loss: 1.2793 - categorical_accuracy: 0.6622 - val_loss: 8.4770 - val_categorical_accuracy: 0.0752\n",
      "Epoch 25/50\n",
      "8973/8973 [==============================] - 328s 37ms/step - loss: 1.1691 - categorical_accuracy: 0.6992 - val_loss: 8.4343 - val_categorical_accuracy: 0.0792\n",
      "Epoch 26/50\n",
      "8973/8973 [==============================] - 325s 36ms/step - loss: 1.0349 - categorical_accuracy: 0.7320 - val_loss: 8.5590 - val_categorical_accuracy: 0.0722\n",
      "Epoch 27/50\n",
      "8973/8973 [==============================] - 327s 36ms/step - loss: 0.9490 - categorical_accuracy: 0.7529 - val_loss: 8.7294 - val_categorical_accuracy: 0.0702\n",
      "Epoch 28/50\n",
      "8973/8973 [==============================] - 331s 37ms/step - loss: 0.8624 - categorical_accuracy: 0.7790 - val_loss: 8.6211 - val_categorical_accuracy: 0.0772\n",
      "Epoch 29/50\n",
      "8973/8973 [==============================] - 330s 37ms/step - loss: 0.7872 - categorical_accuracy: 0.8022 - val_loss: 8.7226 - val_categorical_accuracy: 0.0812\n",
      "Epoch 30/50\n",
      "8973/8973 [==============================] - 328s 37ms/step - loss: 0.7125 - categorical_accuracy: 0.8202 - val_loss: 8.8834 - val_categorical_accuracy: 0.0702\n",
      "Epoch 31/50\n",
      "8973/8973 [==============================] - 327s 36ms/step - loss: 0.6722 - categorical_accuracy: 0.8248 - val_loss: 8.6547 - val_categorical_accuracy: 0.0682\n",
      "Epoch 32/50\n",
      "8973/8973 [==============================] - 326s 36ms/step - loss: 0.5943 - categorical_accuracy: 0.8517 - val_loss: 8.8003 - val_categorical_accuracy: 0.0732\n",
      "Epoch 33/50\n",
      "8973/8973 [==============================] - 324s 36ms/step - loss: 0.5427 - categorical_accuracy: 0.8678 - val_loss: 9.0727 - val_categorical_accuracy: 0.0722\n",
      "Epoch 34/50\n",
      "8973/8973 [==============================] - 327s 36ms/step - loss: 0.5097 - categorical_accuracy: 0.8814 - val_loss: 9.0723 - val_categorical_accuracy: 0.0672\n",
      "Epoch 35/50\n",
      "8973/8973 [==============================] - 329s 37ms/step - loss: 0.4576 - categorical_accuracy: 0.8939 - val_loss: 9.0931 - val_categorical_accuracy: 0.0712\n",
      "Epoch 36/50\n",
      "8973/8973 [==============================] - 333s 37ms/step - loss: 0.4235 - categorical_accuracy: 0.9004 - val_loss: 9.0657 - val_categorical_accuracy: 0.0682\n",
      "Epoch 37/50\n",
      "8973/8973 [==============================] - 329s 37ms/step - loss: 0.4028 - categorical_accuracy: 0.9040 - val_loss: 9.2219 - val_categorical_accuracy: 0.0712\n",
      "Epoch 38/50\n",
      "8973/8973 [==============================] - 328s 37ms/step - loss: 0.3731 - categorical_accuracy: 0.9154 - val_loss: 9.0716 - val_categorical_accuracy: 0.0702\n",
      "Epoch 39/50\n",
      "8973/8973 [==============================] - 335s 37ms/step - loss: 0.3533 - categorical_accuracy: 0.9182 - val_loss: 9.1596 - val_categorical_accuracy: 0.0652\n",
      "Epoch 40/50\n",
      "8973/8973 [==============================] - 346s 39ms/step - loss: 0.3179 - categorical_accuracy: 0.9273 - val_loss: 9.2706 - val_categorical_accuracy: 0.0662\n",
      "Epoch 41/50\n",
      "8973/8973 [==============================] - 347s 39ms/step - loss: 0.3056 - categorical_accuracy: 0.9310 - val_loss: 9.1202 - val_categorical_accuracy: 0.0622\n",
      "Epoch 42/50\n",
      "8973/8973 [==============================] - 338s 38ms/step - loss: 0.2845 - categorical_accuracy: 0.9378 - val_loss: 9.2635 - val_categorical_accuracy: 0.0692\n",
      "Epoch 43/50\n",
      "8973/8973 [==============================] - 336s 37ms/step - loss: 0.2598 - categorical_accuracy: 0.9449 - val_loss: 9.2680 - val_categorical_accuracy: 0.0712\n",
      "Epoch 44/50\n",
      "8973/8973 [==============================] - 330s 37ms/step - loss: 0.2506 - categorical_accuracy: 0.9463 - val_loss: 9.6198 - val_categorical_accuracy: 0.0702\n",
      "Epoch 45/50\n",
      "8973/8973 [==============================] - 334s 37ms/step - loss: 0.2402 - categorical_accuracy: 0.9483 - val_loss: 9.3746 - val_categorical_accuracy: 0.0722\n",
      "Epoch 46/50\n",
      "8973/8973 [==============================] - 304s 34ms/step - loss: 0.2276 - categorical_accuracy: 0.9503 - val_loss: 9.2506 - val_categorical_accuracy: 0.0692\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8973/8973 [==============================] - 289s 32ms/step - loss: 0.2122 - categorical_accuracy: 0.9545 - val_loss: 9.2446 - val_categorical_accuracy: 0.0672\n",
      "Epoch 48/50\n",
      "8973/8973 [==============================] - 286s 32ms/step - loss: 0.2036 - categorical_accuracy: 0.9600 - val_loss: 9.2955 - val_categorical_accuracy: 0.0672\n",
      "Epoch 49/50\n",
      "8973/8973 [==============================] - 287s 32ms/step - loss: 0.2011 - categorical_accuracy: 0.9583 - val_loss: 9.4445 - val_categorical_accuracy: 0.0652\n",
      "Epoch 50/50\n",
      "8973/8973 [==============================] - 281s 31ms/step - loss: 0.1861 - categorical_accuracy: 0.9617 - val_loss: 9.3461 - val_categorical_accuracy: 0.0612\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # minibatch size\n",
    "num_epochs = 50 # number of epochs\n",
    "\n",
    "# callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "#            ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "#                            monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "#fit the model\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "#                  callbacks=callbacks,\n",
    "                 validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "md.save('my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "loading model...\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "# vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    " \n",
    "with open('word_dictt.txt', 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)\n",
    "\n",
    "with open('word_dict.txt', 'rb') as f:\n",
    "    words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model('my_model_generate_sentences.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193201"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The moon, painted with a smiling face hangs over a park at night. A young couple walking past a fence learn on a railing and look up. The moon smiles. They embrace, and the moon's smile gets bigger. They then sit down on a bench by a tree. The moon's view is blocked, causing him to frown. In the last scene, the man fans the woman with his hat because the moon has left the sky and is perched over her shoulder to see everything better.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Plot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a a a a a a a a a a he is walking past and a as his bob himself from a time and they enter the husband of a herself and her to his rival goldberg in the henry of a santa claus\n"
     ]
    }
   ],
   "source": [
    "words_number = 30 # number of words to generate\n",
    "seed_sentences = \"he is walking past \" #seed sentence to start the generating.\n",
    "\n",
    "#initiate sentences\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "#we shate the seed accordingly to the neural netwrok needs:\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "\n",
    "#the, we generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.33)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "#print the whole text\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"he is walking past and a fence learn on a passing and an trusting boone and a thus when he has to onward and the into the moon's as he is a summer that\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a a a a john would always go up the stairs only to see and fight on the santa claus she jack home to found the tramp and time and an she is has to the spread mollie he be be a new york\n"
     ]
    }
   ],
   "source": [
    "words_number = 30 # number of words to generate\n",
    "seed_sentences = \"john would always go up the stairs only to see\" #seed sentence to start the generating.\n",
    "\n",
    "#initiate sentences\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "#we shate the seed accordingly to the neural netwrok needs:\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "\n",
    "#the, we generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.33)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "#print the whole text\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john would always go up the stairs only to see and fight on the santa claus she jack home to found the tramp and time and an she is has to the spread mollie he be be a new york'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
